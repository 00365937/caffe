.build_release/tools/caffe
caffe: command line brew
usage: caffe <command> <args>

commands:
  train           train or finetune a model
  test            score a model
  device_query    show GPU diagnostic information
  time            benchmark model execution time

  Flags from tools/caffe.cpp:
    -gpu (Optional; run in GPU mode on given device IDs separated by ','.Use
      '-gpu all' to run on all available GPUs. The effective training batch
      size is multiplied by the number of devices.) type: string default: ""
    -iterations (The number of iterations to run.) type: int32 default: 50
    -model (The model definition protocol buffer text file..) type: string
      default: ""
    -sighup_effect (Optional; action to take when a SIGHUP signal is received:
      snapshot, stop or none.) type: string default: "snapshot"
    -sigint_effect (Optional; action to take when a SIGINT signal is received:
      snapshot, stop or none.) type: string default: "stop"
    -snapshot (Optional; the snapshot solver state to resume training.)
      type: string default: ""
    -solver (The solver definition protocol buffer text file.) type: string
      default: ""
    -weights (Optional; the pretrained weights to initialize finetuning,
      separated by ','. Cannot be set simultaneously with snapshot.)
      type: string default: ""
.build_release/test/test_all.testbin 1 --gtest_shuffle 
Setting to use device 1
Build Status = -2 ( Err = -11 )
Log: <kernel>:1082:21: warning: initializing '__global float *__attribute__((address_space(16776963)))' with an expression of type 'const __global float *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~
<kernel>:2752:15: error: conflicting types for 'col2im_nd_gpu_kernel'
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:648:15: note: previous definition is here
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:3186:21: warning: initializing '__global double *__attribute__((address_space(16776963)))' with an expression of type 'const __global double *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~

Sources: #ifndef __OPENCL_VERSION__
#define __kernel
#define __global
#define __constant
#define __local
#define get_global_id(x) 0
#define get_global_size(x) 0
#define get_local_id(x) 0
#define get_local_size(x) 0
#define FLT_MAX 0
#define FLT_MIN 0
#define cl_khr_fp64
#define cl_amd_fp64
#define DOUBLE_SUPPORT_AVAILABLE
#define CLK_LOCAL_MEM_FENCE
#define Dtype float
#define barrier(x)
#define atomic_cmpxchg(x, y, z) x
#endif

#define CONCAT(A,B) A##_##B
#define TEMPLATE(name,type) CONCAT(name,type)

#define TYPE_FLOAT 1
#define TYPE_DOUBLE 2

#if defined(cl_khr_fp64)
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#endif

#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#define ATOMICS_64_AVAILABLE
#endif

#define Dtype float

#define TYPE TYPE_FLOAT

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#ifdef DOUBLE_SUPPORT_AVAILABLE

#undef Dtype

#define Dtype double

#undef TYPE

#define TYPE TYPE_DOUBLE

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#endif


Build Status = -2 ( Err = -11 )
Log: <kernel>:1082:21: warning: initializing '__global float *__attribute__((address_space(16776963)))' with an expression of type 'const __global float *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~
<kernel>:2752:15: error: conflicting types for 'col2im_nd_gpu_kernel'
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:648:15: note: previous definition is here
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:3186:21: warning: initializing '__global double *__attribute__((address_space(16776963)))' with an expression of type 'const __global double *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~

Sources: #ifndef __OPENCL_VERSION__
#define __kernel
#define __global
#define __constant
#define __local
#define get_global_id(x) 0
#define get_global_size(x) 0
#define get_local_id(x) 0
#define get_local_size(x) 0
#define FLT_MAX 0
#define FLT_MIN 0
#define cl_khr_fp64
#define cl_amd_fp64
#define DOUBLE_SUPPORT_AVAILABLE
#define CLK_LOCAL_MEM_FENCE
#define Dtype float
#define barrier(x)
#define atomic_cmpxchg(x, y, z) x
#endif

#define CONCAT(A,B) A##_##B
#define TEMPLATE(name,type) CONCAT(name,type)

#define TYPE_FLOAT 1
#define TYPE_DOUBLE 2

#if defined(cl_khr_fp64)
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#endif

#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#define ATOMICS_64_AVAILABLE
#endif

#define Dtype float

#define TYPE TYPE_FLOAT

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#ifdef DOUBLE_SUPPORT_AVAILABLE

#undef Dtype

#define Dtype double

#undef TYPE

#define TYPE TYPE_DOUBLE

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#endif


Build Status = -2 ( Err = -11 )
Log: <kernel>:1082:21: warning: initializing '__global float *__attribute__((address_space(16776963)))' with an expression of type 'const __global float *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~
<kernel>:2752:15: error: conflicting types for 'col2im_nd_gpu_kernel'
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:648:15: note: previous definition is here
__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
              ^
<kernel>:3186:21: warning: initializing '__global double *__attribute__((address_space(16776963)))' with an expression of type 'const __global double *' discards qualifiers
    __global Dtype* top_diff_off = top_diff + offset;
                    ^              ~~~~~~~~~~~~~~~~~

Sources: #ifndef __OPENCL_VERSION__
#define __kernel
#define __global
#define __constant
#define __local
#define get_global_id(x) 0
#define get_global_size(x) 0
#define get_local_id(x) 0
#define get_local_size(x) 0
#define FLT_MAX 0
#define FLT_MIN 0
#define cl_khr_fp64
#define cl_amd_fp64
#define DOUBLE_SUPPORT_AVAILABLE
#define CLK_LOCAL_MEM_FENCE
#define Dtype float
#define barrier(x)
#define atomic_cmpxchg(x, y, z) x
#endif

#define CONCAT(A,B) A##_##B
#define TEMPLATE(name,type) CONCAT(name,type)

#define TYPE_FLOAT 1
#define TYPE_DOUBLE 2

#if defined(cl_khr_fp64)
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#elif defined(cl_amd_fp64)
#pragma OPENCL EXTENSION cl_amd_fp64 : enable
#define DOUBLE_SUPPORT_AVAILABLE
#endif

#if defined(cl_khr_int64_base_atomics)
#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable
#define ATOMICS_64_AVAILABLE
#endif

#define Dtype float

#define TYPE TYPE_FLOAT

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#ifdef DOUBLE_SUPPORT_AVAILABLE

#undef Dtype

#define Dtype double

#undef TYPE

#define TYPE TYPE_DOUBLE

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(relu_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out,
                                           Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;
  }
}

__kernel void TEMPLATE(relu_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff,
                                            Dtype negative_slope) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);
  }
}

__kernel void TEMPLATE(tanh_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = tanh(in[index]);
  }
}

__kernel void TEMPLATE(tanh_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* out_data,
                                            __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype tanhx = out_data[index];
    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);
  }
}

__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = 1. / (1. + exp(-in[index]));
  }
}

__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int n,
                                               __global const Dtype* in_diff,
                                               __global const Dtype* out_data,
                                               __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const Dtype sigmoid_x = out_data[index];
    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);
  }
}

__kernel void TEMPLATE(threshold,Dtype)(const int n, const Dtype threshold,
                                        __global const Dtype* in,
                                        __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] > threshold ? 1 : 0;
  }
}

__kernel void TEMPLATE(prelu_forward,Dtype)(const int n, const int channels,
                                            const int dim,
                                            __global const Dtype* in,
                                            __global Dtype* out,
                                            __global const Dtype* slope_data,
                                            const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];
  }
}

__kernel void TEMPLATE(prelu_backward,Dtype)(const int n, const int channels,
                                             const int dim,
                                             __global const Dtype* in_diff,
                                             __global const Dtype* in_data,
                                             __global Dtype* out_diff,
                                             __global const Dtype* slope_data,
                                             const int div_factor) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int c = (index / dim) % channels / div_factor;
    out_diff[index] = in_diff[index]
        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);
  }
}

__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int n,
                                                   __global const Dtype* in_diff, const int in_diff_off,
                                                   __global const Dtype* in_data, const int in_data_off,
                                                   __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index + in_diff_off] * in_data[index + in_data_off] * (in_data[index + in_data_off] <= 0);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(gpu_set,Dtype)(const int n, const Dtype alpha, __global Dtype* y) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(bnll_forward,Dtype)(const int n,
                                           __global const Dtype* in,
                                           __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] =
        in[index] > 0 ?
            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));
  }
}

__kernel void TEMPLATE(bnll_backward,Dtype)(const int n,
                                            __global const Dtype* in_diff,
                                            __global const Dtype* in_data,
                                            __global Dtype* out_diff) {
  Dtype kBNLL_THRESHOLD = 50.;
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));
    out_diff[index] = in_diff[index] * expval / (expval + 1.);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* out) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    float maxval = -FLT_MAX;
    for (int c = 0; c < channels; ++c) {
      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);
    }
    out[index] = maxval;
  }
}

__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int count, const int num,
                                        const int channels,
                                        const int spatial_dim,
                                        __global const Dtype* channel_max,
                                        __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] -= channel_max[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_exp,Dtype)(const int count, __global const Dtype* data,
                           __global Dtype* out) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    out[index] = exp(data[index]);
  }
}

__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data,
                                   __global Dtype* channel_sum) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype sum = 0;
    for (int c = 0; c < channels; ++c) {
      sum += data[(n * channels + c) * spatial_dim + s];
    }
    channel_sum[index] = sum;
  }
}

__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int count, const int num,
                                   const int channels, const int spatial_dim,
                                   __global const Dtype* channel_sum,
                                   __global Dtype* data) {
  for (int index = get_global_id(0); index < count;
      index += get_global_size(0)) {
    int n = index / channels / spatial_dim;
    int s = index % spatial_dim;
    data[index] /= channel_sum[n * spatial_dim + s];
  }
}

__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int num, const int channels,
                                   const int spatial_dim,
                                   __global const Dtype* data_1,
                                   __global const Dtype* data_2,
                                   __global Dtype* channel_dot) {
  for (int index = get_global_id(0); index < num * spatial_dim; index +=
      get_global_size(0)) {
    int n = index / spatial_dim;
    int s = index % spatial_dim;
    Dtype dot = 0;
    for (int c = 0; c < channels; ++c) {
      dot += (data_1[(n * channels + c) * spatial_dim + s]
          * data_2[(n * channels + c) * spatial_dim + s]);
    }
    channel_dot[index] = dot;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(concat,Dtype)(const int nthreads, __global const Dtype* in_data,
                                     const int forward, const int num_concats,
                                     const int concat_size,
                                     const int top_concat_axis,
                                     const int bottom_concat_axis,
                                     const int offset_concat_axis,
                                     __global Dtype* out_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_concat_size = concat_size * bottom_concat_axis;
    const int concat_num = index / total_concat_size;
    const int concat_index = index % total_concat_size;
    const int top_index = concat_index
        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;
    if (forward == 1) {
      out_data[top_index] = in_data[index];
    } else {
      out_data[index] = in_data[top_index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(cll_backward,Dtype)(const int count, const int channels,
                            const Dtype margin, const int legacy_version,
                            const Dtype alpha, __global const Dtype* y,
                            __global const Dtype* diff, __global const Dtype* dist_sq,
                            __global Dtype *bottom_diff) {
  for (int i = get_global_id(0); i < count;
      i += get_global_size(0)) {
    int n = i / channels;  // the num index, to access y and dist_sq
    if ((int)(y[n])) {  // similar pairs
      bottom_diff[i] = alpha * diff[i];
    } else {  // dissimilar pairs
      Dtype mdist = 0.0;
      Dtype beta = 0.0;
      if (legacy_version == 1) {
        mdist = (margin - dist_sq[n]);
        beta = -alpha;
      } else {
        Dtype dist = sqrt(dist_sq[n]);
        mdist = (margin - dist);
        beta = -alpha * mdist / (dist + 1e-4) * diff[i];
      }
      if (mdist > 0.0) {
        bottom_diff[i] = beta;
      } else {
        bottom_diff[i] = 0;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(dropout_forward,Dtype)(const int n,
                                              __global const Dtype* in,
                                              __global const unsigned int* mask,
                                              const unsigned int threshold,
                                              const Dtype scale,
                                              __global Dtype* out) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;
  }
}

__kernel void TEMPLATE(dropout_backward,Dtype)(
    const int n, __global const Dtype* in_diff,
    __global const unsigned int* mask, const unsigned int threshold,
    const Dtype scale,
    __global Dtype* out_diff) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    out_diff[index] = in_diff[index] * scale * (mask[index] > threshold);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(eltwise_max_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data_a,
    __global const Dtype* bottom_data_b, const int blob_idx,
    __global Dtype* top_data,
    __global int* mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    if (bottom_data_a[index] > bottom_data_b[index]) {
      // only update for very first bottom_data blob (blob_idx == 0)
      if (blob_idx == 0) {
        maxval = bottom_data_a[index];
        top_data[index] = maxval;
        maxidx = blob_idx;
        mask[index] = maxidx;
      }
    } else {
      maxval = bottom_data_b[index];
      top_data[index] = maxval;
      maxidx = blob_idx + 1;
      mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int nthreads,
                                                   __global const Dtype* top_diff,
                                                   const int blob_idx,
                                                   __global const int* mask,
                                                   __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    Dtype gradient = 0;
    if (mask[index] == blob_idx) {
      gradient += top_diff[index];
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(embed_forward,Dtype)(const int nthreads,
                                            __global const Dtype* bottom_data,
                                            __global const Dtype* weight,
                                            const int M, const int N,
                                            const int K,
                                            __global Dtype* top_data) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
      const int n = top_index / N;
      const int d = top_index % N;
      const int index = (int)(bottom_data[n]);
      const int weight_index = index * N + d;
      top_data[top_index] = weight[weight_index];
    }
  }

// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html
#if (TYPE == TYPE_FLOAT)
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned int intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned int intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif

#if (TYPE == TYPE_DOUBLE)
#ifdef ATOMICS_64_AVAILABLE
inline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {
    union {
        unsigned long intVal;
        Dtype floatVal;
    } newVal;
    union {
        unsigned long intVal;
        Dtype floatVal;
    } prevVal;
    do {
        prevVal.floatVal = *source;
        newVal.floatVal = prevVal.floatVal + operand;
    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);
}

__kernel void TEMPLATE(embed_backward,Dtype)(const int nthreads, __global const Dtype* bottom_data,
    __global const Dtype* top_diff, const int M, const int N, const int K,
    __global Dtype* weight_diff) {
  for (int top_index = get_global_id(0); top_index < nthreads;
      top_index += get_global_size(0)) {
    const int n = top_index / N;
    const int d = top_index % N;
    const int index = (int)(bottom_data[n]);
    const int weight_index = index * N + d;

    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));
  }
}
#endif
#endif

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(fillbuffer,Dtype)(const int n, const char alpha, __global char* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

__kernel void TEMPLATE(fill,Dtype)(const int n, const Dtype alpha, __global Dtype* x,
                                   const int offx) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    x[index + offx] = alpha;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col,Dtype)(const int n, __global const Dtype* data_im, const int data_im_off,
    const int height, const int width, const int kernel_h, const int kernel_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_col, const int data_col_off) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col + data_col_off;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_im_off;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < kernel_h; ++i) {
      for (int j = 0; j < kernel_w; ++j) {
        int h = h_in + i;
        int w = w_in + j;
        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?
            data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }
}

__kernel void TEMPLATE(col2im,Dtype)(const int n, __global const Dtype* data_col, const int data_col_off,
    const int height, const int width, const int channels,
    const int patch_h, const int patch_w,
    const int pad_h, const int pad_w,
    const int stride_h, const int stride_w,
    const int height_col, const int width_col,
    __global Dtype* data_im, const int data_im_off) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;
    int w_col_end = min(w / stride_w + 1, width_col);
    int h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;
    int h_col_end = min(h / stride_h + 1, height_col);
    int offset = data_col_off +
        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;
    int coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;
    int coeff_w_col = (1 - stride_w * height_col * width_col);
    for (int h_col = h_col_start; h_col < h_col_end; ++h_col) {
      for (int w_col = w_col_start; w_col < w_col_end; ++w_col) {
        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];
      }
    }
    data_im[index + data_im_off] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_nd, Dtype)(const int n, const int num_axes,
                                     __global const Dtype* data_im,
                                     const int data_off,
                                     __global const int* im_shape,
                                     __global const int* col_shape,
                                     __global const int* kernel_shape,
                                     __global const int* pad,
                                     __global const int* stride,
                                     __global Dtype* data_col,
                                     const int data_col_off) {

  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + channel_out;
    __global const Dtype* data_im_ptr = data_im + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }
      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        const int d_max = kernel_shape[i];
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          ++d_iter[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}



__kernel void col2im_nd_gpu_kernel(const int n, const int num_axes,
                                   __global const Dtype* data_col,
                                   __global const int* im_shape,
                                   __global const int* col_shape,
                                   __global const int* kernel_shape,
                                   __global const int* pad,
                                   __global const int* stride,
                                   __global Dtype* data_im) {
  int d_im[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);
      if (d_col_start[i] >= d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int kernel_shape_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += (d_im[i] - d_col_iter[i] * stride[i])
            * kernel_shape_prod;
        kernel_shape_prod *= kernel_shape[i];
      }
      final_offset += kernel_shape_prod * channel_im;
      for (int i = 0; i < num_axes; ++i) {
        final_offset *= col_shape[i + 1];
        final_offset += d_col_iter[i];
      }
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        const int d_max = d_col_end[i];
        if (d_col_iter[i] == d_max - 1) {
          d_col_iter[i] = d_col_start[i];
        } else {  // d_col_iter[i] < d_max - 1
          ++d_col_iter[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int n, const int num_axes,
                                        __global const Dtype* data_im,
                                        const int data_off,
                                        __global const int* im_shape,
                                        __global const int* col_shape,
                                        __global const int* kernel_shape,
                                        __global const int* pad,
                                        __global const int* stride,
                                        __global const int* kstride,
                                        __global Dtype* data_col,
                                        const int data_col_off) {
  int d_temp[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_in = index;
    int channel_out = 1;
    for (i = num_axes - 1; i >= 0; --i) {
      d_temp[i] = channel_in % col_shape[i + 1];
      channel_in /= col_shape[i + 1];
      channel_out *= kernel_shape[i];
    }
    channel_out *= channel_in;
    int data_col_inc = 1;
    for (i = 0; i < num_axes; ++i) {
      channel_out *= col_shape[i + 1];
      channel_out += d_temp[i];
      d_temp[i] = d_temp[i] * stride[i] - pad[i];
      channel_in *= im_shape[i + 1];
      channel_in += d_temp[i];
      data_col_inc *= col_shape[i + 1];
      d_iter[i] = 0;
    }
    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;
    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;
    bool incremented;
    do {
      bool in_range = true;
      for (i = 0; i < num_axes; ++i) {
        const int d_iter_im = d_iter[i] + d_temp[i];
        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];
        if (!in_range) {
          break;
        }
      }

      // Write column data
      if (in_range) {
        int data_im_offset = d_iter[0];
        for (i = 1; i < num_axes; ++i) {
          data_im_offset *= im_shape[i + 1];
          data_im_offset += d_iter[i];
        }
        *data_col_ptr = data_im_ptr[data_im_offset];
      } else {
        *data_col_ptr = 0;
      }

      data_col_ptr += data_col_inc;
      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        // Old: const int d_max = kernel_shape[i];
        // New (strided, limit is the external kernel size):
        const int d_max = (kernel_shape[i] - 1) * kstride[i] + 1;
        if (d_iter[i] == d_max - 1) {
          d_iter[i] = 0;
        } else {  // d_iter[i] < d_max - 1
          // Old: ++d_iter[i];
          // New (strided, increment by the stride each time):
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    } while (incremented);  // do
  }
}

__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int n, const int num_axes,
                                  __global const Dtype* data_col,
                                    const int data_col_off,
                                  __global const int* im_shape,
                                  __global const int* col_shape,
                                  __global const int* kernel_shape,
                                  __global const int* pad,
                                  __global const int* stride,
                                  __global const int* kstride,
                                  __global Dtype* data_im,
                                  const int data_off) {
  int d_im[6];
  int d_col_size[6];
  int d_col_iter[6];
  int d_col_start[6];
  int d_col_end[6];
  int d_ext_patch[6];
  int d_idx[6];

  for (int i = num_axes - 1; i >= 0; --i) {
    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;
    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])
        / stride[i] + 1;
  }

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // Initialize channel_in, computed in the loop below, with intermediate
    // computations used to compute the spatial indices.
    int channel_im = index;
    // Calculate d_im (image dimensions).
    for (int i = num_axes - 1; i >= 0; --i) {
      d_im[i] = channel_im % im_shape[i + 1] + pad[i];
      channel_im /= im_shape[i + 1];
    }
    // Calculate col start/end indices.
    bool done = false;
    for (int i = 0; i < num_axes; ++i) {
      // Old:
      /*d_col_start[i] = d_col_iter[i] =
          (d_im[i] < kernel_shape[i]) ?
          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;
      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/
      // New:
      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?
          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;
      d_col_iter[i] = d_col_start[i];
      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?
          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])
          % kstride[i] : d_im[i];
      if (d_col_start[i] > d_col_end[i]) {
        // Skip computation if the dimension is 0 at any spatial axis --
        // final val will be 0.
        data_im[index] = 0;
        done = true;
        break;  // for (int i = 0; i < num_axes; ++i)
      }
    }
    if (done) {
      continue;
    }
    // Loop over the col to compute the output val.
    Dtype val = 0;
    bool incremented = true;
    do {
      // Compute the final offset.
      int final_offset = 0;
      int coeff_prod = 1;
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset +=  d_col_iter[i] * coeff_prod;
        coeff_prod *= d_col_size[i];
      }
      for (int i = num_axes - 1; i >= 0; --i) {
        final_offset += d_idx[i] * coeff_prod;
        coeff_prod *= kernel_shape[i];
      }
      final_offset += channel_im * coeff_prod;
      val += data_col[final_offset];
      incremented = false;
      for (int i = num_axes - 1; i >= 0; --i) {
        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {
          d_col_iter[i] = d_col_start[i];
          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];
        } else {  // d_col_iter[i] <= d_max - kstride[1]
          d_col_iter[i] += kstride[i];
          --d_idx[i];
          incremented = true;
          break;  // for (int i = num_axes - 1; i >= 0; --i)
        }
      }  // for (int i = num_axes - 1; i >= 0; --i)
    }  while (incremented);
    data_im[index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(im2col_sk,Dtype)(const int n,
                                        __global const Dtype* data_im,
                                        const int data_offset, const int height,
                                        const int width, const int kernel_h,
                                        const int kernel_w,
                                        const int ext_kernel_h,
                                        const int ext_kernel_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_col) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int w_out = index % width_col;
    int h_index = index / width_col;
    int h_out = h_index % height_col;
    int channel_in = h_index / height_col;
    int channel_out = channel_in * kernel_h * kernel_w;
    int h_in = h_out * stride_h - pad_h;
    int w_in = w_out * stride_w - pad_w;
    __global Dtype* data_col_ptr = data_col;
    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;
    __global const Dtype* data_im_ptr = data_im + data_offset;
    data_im_ptr += (channel_in * height + h_in) * width + w_in;
    for (int i = 0; i < ext_kernel_h; i += kstride_h) {
      for (int j = 0; j < ext_kernel_w; j += kstride_w) {
        int h = h_in + i;
        int w = w_in + j;
        (*data_col_ptr) =
            (h >= 0 && w >= 0 && h < height && w < width) ?
                data_im_ptr[i * width + j] : 0;
        data_col_ptr += height_col * width_col;
      }
    }
  }

}

__kernel void TEMPLATE(col2im_sk,Dtype)(const int n,
                                        __global const Dtype* data_col,
                                        const int height, const int width,
                                        const int channels, const int patch_h,
                                        const int patch_w,
                                        const int ext_patch_h,
                                        const int ext_patch_w, const int pad_h,
                                        const int pad_w, const int stride_h,
                                        const int stride_w, const int kstride_h,
                                        const int kstride_w,
                                        const int height_col,
                                        const int width_col,
                                        __global Dtype* data_im,
                                        const int data_offset) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    Dtype val = 0;
    int w = index % width + pad_w;
    int h = (index / width) % height + pad_h;
    int c = index / (width * height);
    // compute the start and end of the output
    int width_col_1 = width_col - 1;
    int height_col_1 = height_col - 1;
    int w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;
    int w_col_end =
        (w >= width_col) ?
            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;
    int h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;
    int h_col_end =
        (h >= height_col) ?
            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;
    int w_num = (w - w_col_start) / kstride_w;
    int h_num = (h - h_col_start) / kstride_h;

    int coeff_w_idx = height_col * width_col;
    int coeff_h_idx = patch_w * coeff_w_idx;
    int offset = c * patch_h * coeff_h_idx;
    for (int h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=
        kstride_h, --h_idx) {
      for (int w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=
          kstride_w, --w_idx) {
        //int c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;
        //int c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;
        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];
        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx
            + h_col * width_col + w_col];
      }
    }

    data_im[data_offset + index] = val;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int nthreads,
                                                 __global const Dtype* in,
                                                 __global const Dtype* scale,
                                                 const Dtype negative_beta,
                                                 __global Dtype* out) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    out[index] = in[index] * pow(scale[index], negative_beta);
  }
}

__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int nthreads, __global const Dtype* in,
                             const int num, const int channels,
                             const int height, const int width, const int size,
                             const Dtype alpha_over_size, const Dtype k,
                             __global Dtype* const scale) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* in_off = in + offset;
    __global Dtype* scale_off = scale + offset;
    int head = 0;
    const int pre_pad = (size - 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_scale = 0;
    // fill the scale at [n, :, h, w]
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_scale += in_off[head * step] * in_off[head * step];
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_scale -= in_off[(head - size) * step]
            * in_off[(head - size) * step];
      }
      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;
      ++head;
    }
  }
}

__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int nthreads,
                               __global const Dtype* bottom_data,
                               __global const Dtype* top_data,
                               __global const Dtype* scale,
                               __global const Dtype* top_diff, const int num,
                               const int channels, const int height,
                               const int width, const int size,
                               const Dtype negative_beta,
                               const Dtype cache_ratio,
                               __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int n = index / width / height;
    const int offset = (n * channels * height + h) * width + w;
    const int step = height * width;
    __global const Dtype* bottom_off = bottom_data + offset;
    __global const Dtype* top_off = top_data + offset;
    __global const Dtype* scale_off = scale + offset;
    __global Dtype* top_diff_off = top_diff + offset;
    __global Dtype* bottom_diff_off = bottom_diff + offset;
    int head = 0;
    const int pre_pad = size - (size + 1) / 2;
    const int post_pad = size - pre_pad - 1;
    Dtype accum_ratio = 0;
    // accumulate values
    while (head < post_pad && head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      ++head;
    }
    // both add and subtract
    while (head < channels) {
      accum_ratio += top_diff_off[head * step] * top_off[head * step]
          / scale_off[head * step];
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
    // subtract only
    while (head < channels + post_pad) {
      if (head - size >= 0) {
        accum_ratio -= top_diff_off[(head - size) * step]
            * top_off[(head - size) * step] / scale_off[(head - size) * step];
      }
      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)
          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)
          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;
      ++head;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(mul,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] * b[index + offb];
  }
}

__kernel void TEMPLATE(div,Dtype)(const int n, __global const Dtype* a,
                                  const int offa,
                                  __global Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = a[index + offa] / b[index + offb];
  }
}

__kernel void TEMPLATE(add_scalar,Dtype)(const int N, const Dtype alpha,
__global Dtype* Y,
                                         const int offY) {
  for (int index = get_global_id(0); index < N; index += get_global_size(0)) {
    Y[offY + index] += alpha;
  }
}

__kernel void TEMPLATE(add,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] + b[offb + index];
  }
}

__kernel void TEMPLATE(sub,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global const Dtype* b,
                                  const int offb, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = a[offa + index] - b[offb + index];
  }
}

__kernel void TEMPLATE(abs,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = fabs((Dtype)(a[offa + index]));
  }
}

__kernel void TEMPLATE(exp,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = exp(a[offa + index]);
  }
}

__kernel void TEMPLATE(log,Dtype)(const int n, __global const Dtype* a,
                                  const int offa, __global Dtype* y,
                                  const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[offy + index] = log(a[offa + index]);
  }
}

__kernel void TEMPLATE(powx,Dtype)(const int n, __global const Dtype* a,
                                   const int offa, Dtype alpha,
                                   __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    if(alpha == 2.0) {
      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);
    } else {
      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);
    }
  }
}

__kernel void TEMPLATE(sign,Dtype)(const int n, __global const Dtype* x,
                                   const int offx, __global Dtype* y,
                                   const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = (0.0 < x[index + offx])
        - (x[index + offx] < 0.0);
  }
}

__kernel void TEMPLATE(sgnbit,Dtype)(const int n, __global const Dtype* x,
                                     const int offx, __global Dtype* y,
                                     const int offy) {
  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    y[index + offy] = signbit(x[index + offx]);
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(merge_copy_forward, Dtype)(
    const int nthreads, __global const Dtype* bottom_a, const int forward_a,
    __global const Dtype* bottom_b, const int forward_b,
    __global Dtype* top,
    int num, int channels_a, int channels_b, int height_a, int width_a,
    int height_b, int width_b) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      top[index] = forward_a == 1 ? bottom_a[aidx] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      top[index] = forward_b == 1 ? bottom_b[bidx] : 0;
    }
  }

}

__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int nthreads,
__global Dtype* bottom_a,
                                                  int backward_a,
                                                  __global Dtype* bottom_b,
                                                  int backward_b,
                                                  __global const Dtype* top,
                                                  int num, int channels_a,
                                                  int channels_b, int height_a,
                                                  int width_a, int height_b,
                                                  int width_b) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int batch_id = index / ((channels_a + channels_b) * height_a * width_a);

    int pad_h = (height_b - height_a) / 2;
    int pad_w = (width_b - width_a) / 2;

    int bottom_id = ((index
        - batch_id * (channels_a + channels_b) * height_a * width_a)
        / (channels_a * height_a * width_a)) % 2;

    int h = ((index / width_a) % height_a);
    int w = (index % width_a);

    if (bottom_id == 0) {
      int channel_id = (index / ((width_a * height_a)) % channels_a);
      int aidx = ((((batch_id) * channels_a + channel_id) * height_a + h)
          * width_a + w);
      bottom_a[aidx] = backward_a == 1 ? top[index] : 0;
    } else {
      int channel_id = (index / ((width_a * height_a)) % channels_b);
      int bidx = (((batch_id) * channels_b + channel_id) * height_b * width_b)
          + width_b * (h + pad_h) + pad_w + w;
      bottom_b[bidx] = backward_b == 1 ? top[index] : 0;
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w,
    __global Dtype* top_data,
    const int use_mask, __global int* mask, __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    const int hend = min(hstart + kernel_h, height);
    const int wend = min(wstart + kernel_w, width);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        if (bottom_slice[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_slice[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(ave_pool_forward,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w, const int pad_h,
    const int pad_w, __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int pw = index % pooled_width;
      const int ph = (index / pooled_width) % pooled_height;
      const int c = (index / pooled_width / pooled_height) % channels;
      const int n = index / pooled_width / pooled_height / channels;
      int hstart = ph * stride_h - pad_h;
      int wstart = pw * stride_w - pad_w;
      int hend = min(hstart + kernel_h, height + pad_h);
      int wend = min(wstart + kernel_w, width + pad_w);
      const int pool_size = (hend - hstart) * (wend - wstart);
      hstart = max(hstart, 0);
      wstart = max(wstart, 0);
      hend = min(hend, height);
      wend = min(wend, width);
      Dtype aveval = 0;
      __global const Dtype* bottom_slice = bottom_data
          + (n * channels + c) * height * width;
      for (int h = hstart; h < hend; ++h) {
        for (int w = wstart; w < wend; ++w) {
          aveval += bottom_slice[h * width + w];
        }
      }
      top_data[index] = aveval / pool_size;
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* rand_idx,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
      }
    }
    const float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_slice[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(
    const int nthreads, __global const Dtype* const bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int stride_h, const int stride_w,
    __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int pw = index % pooled_width;
    const int ph = (index / pooled_width) % pooled_height;
    const int c = (index / pooled_width / pooled_height) % channels;
    const int n = index / pooled_width / pooled_height / channels;
    const int hstart = ph * stride_h;
    const int hend = min(hstart + kernel_h, height);
    const int wstart = pw * stride_w;
    const int wend = min(wstart + kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_slice = bottom_data
        + (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        cumsum += bottom_slice[h * width + w];
        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }
}

__kernel void TEMPLATE(max_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int use_mask,
                                                __global const int* mask,
                                                __global const Dtype* top_mask,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart =
        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;
    const int phend = min((h + pad_h) / stride_h + 1, pooled_height);
    const int pwstart =
        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;
    const int pwend = min((w + pad_w) / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    const int offset = (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff + offset;
    if (use_mask == 1) {
      __global const int* mask_slice = mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    } else {
      __global const Dtype* top_mask_slice = top_mask + offset;
      for (int ph = phstart; ph < phend; ++ph) {
        for (int pw = pwstart; pw < pwend; ++pw) {
          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_slice[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int nthreads,
                                                __global const Dtype* top_diff,
                                                const int num,
                                                const int channels,
                                                const int height,
                                                const int width,
                                                const int pooled_height,
                                                const int pooled_width,
                                                const int kernel_h,
                                                const int kernel_w,
                                                const int stride_h,
                                                const int stride_w,
                                                const int pad_h,
                                                const int pad_w,
                                                __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width + pad_w;
    const int h = (index / width) % height + pad_h;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* const top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        // figure out the pooling size
        int hstart = ph * stride_h - pad_h;
        int wstart = pw * stride_w - pad_w;
        int hend = min(hstart + kernel_h, height + pad_h);
        int wend = min(wstart + kernel_w, width + pad_w);
        int pool_size = (hend - hstart) * (wend - wstart);
        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(sto_pool_backward,Dtype)(
    const int nthreads, __global const Dtype* rand_idx,
    __global const Dtype* const top_diff, const int num, const int channels,
    const int height, const int width, const int pooled_height,
    const int pooled_width, const int kernel_h, const int kernel_w,
    const int stride_h, const int stride_w, __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    const int w = index % width;
    const int h = (index / width) % height;
    const int c = (index / width / height) % channels;
    const int n = index / width / height / channels;
    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;
    const int phend = min(h / stride_h + 1, pooled_height);
    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;
    const int pwend = min(w / stride_w + 1, pooled_width);
    Dtype gradient = 0;
    __global const Dtype* rand_idx_slice = rand_idx
        + (n * channels + c) * pooled_height * pooled_width;
    __global const Dtype* top_diff_slice = top_diff
        + (n * channels + c) * pooled_height * pooled_width;
    for (int ph = phstart; ph < phend; ++ph) {
      for (int pw = pwstart; pw < pwend; ++pw) {
        gradient += top_diff_slice[ph * pooled_width + pw]
            * (index == (int) (rand_idx_slice[ph * pooled_width + pw]));
      }
    }
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int n,
                                                   const int num_axes,
                                                   const __global Dtype* bottom_data,
                                                   const int channels,
                                                   __global const int* size,
                                                   __global const int* pooled_size,
                                                   __global const int* kernel_size,
                                                   __global const int* ext_kernel_size,
                                                   __global const int* stride,
                                                   __global const int* kstride,
                                                   __global const int* pad,
                                                   __global Dtype* top_data,
                                                   const int use_mask,
                                                   __global int* mask, __global Dtype* top_mask) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = index % pooled_size[i];
      d_start[i] = d_idx[i] * stride[i] - pad[i];
      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);
      d_start[i] = max(d_start[i], 0);
      num /= pooled_size[i];
      offset *= size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] >= d_end[i]) {
        top_data[index] = -FLT_MAX;
        if (use_mask) {
          mask[index] = -1;
        } else {
          top_mask[index] = -1;
        }
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    int final_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      int size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * size_prod;
        size_prod *= size[i];
      }

      if (bottom_data[final_offset] > maxval) {
        maxidx = final_offset;
        maxval = bottom_data[maxidx];
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] >= d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);

    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}


__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int n,
                                                    const int num_axes,
                                                    const __global Dtype* top_diff,
                                                    const int use_mask,
                                                    __global const int* mask,
                                                    __global const Dtype* top_mask,
                                                    const int channels,
                                                    __global const int* size,
                                                    __global const int* pooled_size,
                                                    __global const int* kernel_size,
                                                    __global const int* ext_kernel_size,
                                                    __global const int* stride,
                                                    __global const int* kstride,
                                                    __global const int* pad,
                                                    __global Dtype* bottom_diff) {
  int d_idx[6];
  int d_start[6];
  int d_end[6];
  int d_iter[6];
  int i;

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    // find out the local index
    // find out the local offset
    int offset = 1;
    int num = index;
    for (i = num_axes - 1; i >= 0; --i) {
      d_idx[i] = num % size[i];
      d_start[i] = (d_idx[i] < ext_kernel_size[i]) ?
          d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;
      d_end[i] = (d_idx[i] >= pooled_size[i]) ?
          (pooled_size[i] - 1) - (pooled_size[i] - 1 - d_start[i]) %
          kstride[i] : d_idx[i];
      num /= size[i];
      offset *= pooled_size[i];
      d_iter[i] = d_start[i];

      if (d_start[i] > d_end[i]) {
        bottom_diff[index] = 0;
        return;
      }
    }
    int chan = num % channels;
    num /= channels;
    offset *= (num * channels + chan);

    Dtype gradient = 0;
    int final_offset = 0;
    int im_offset = 0;

    bool incremented;
    do {
      final_offset = offset;
      im_offset = 0;
      int size_prod = 1;
      int pooled_size_prod = 1;
      for (i = num_axes - 1; i >= 0; --i) {
        final_offset += d_iter[i] * pooled_size_prod;
        im_offset += d_idx[i] * size_prod;
        size_prod *= size[i];
        pooled_size_prod *= pooled_size[i];
      }

      if (use_mask) {
        if (mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      } else {
        if (top_mask[final_offset] == im_offset) {
          gradient += top_diff[final_offset];
        }
      }

      incremented = false;
      for (i = num_axes - 1; i >= 0; --i) {
        if (d_iter[i] > d_end[i] - kstride[i]) {
          d_iter[i] = d_start[i];
        } else {
          d_iter[i] += kstride[i];
          incremented = true;
          break;
        }
      }
    } while (incremented);
    bottom_diff[index] = gradient;
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int nthreads,
__global Dtype* bottom_data,
                                                  const int num,
                                                  const int channels,
                                                  const int height,
                                                  const int width,
                                                  const int pooled_height,
                                                  const int pooled_width,
                                                  const int kernel_h,
                                                  const int kernel_w,
                                                  const int ext_kernel_h,
                                                  const int ext_kernel_w,
                                                  const int stride_h,
                                                  const int stride_w,
                                                  const int kstride_h,
                                                  const int kstride_w,
                                                  const int pad_h,
                                                  const int pad_w,
                                                  __global Dtype* top_data,
                                                  const int use_mask,
                                                  __global int* mask,
                                                  __global Dtype* top_mask) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height);
    int wend = min(wstart + ext_kernel_w, width);
    hstart = max(hstart, (int) 0);
    wstart = max(wstart, (int) 0);
    Dtype maxval = -FLT_MAX;
    int maxidx = -1;
    __global Dtype* bottom_data_ptr = bottom_data
        + (n * channels + c) * height * width;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        if (bottom_data_ptr[h * width + w] > maxval) {
          maxidx = h * width + w;
          maxval = bottom_data_ptr[maxidx];
        }
      }
    }
    top_data[index] = maxval;
    if (use_mask == 1) {
      mask[index] = maxidx;
    } else {
      top_mask[index] = maxidx;
    }
  }
}

__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(
    const int nthreads, __global const Dtype* top_diff, const int use_mask,
    __global const int* mask, __global const Dtype* top_mask, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* bottom_diff) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    __global const int* mask_ptr = mask;
    __global const Dtype* top_diff_ptr = top_diff;

// find out the local index
// find out the local offset
    int w = index % width;
    int h = (index / width) % height;
    int c = (index / width / height) % channels;
    int n = index / width / height / channels;

    int pooled_height_1 = pooled_height - 1;
    int pooled_width_1 = pooled_width - 1;
    int phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;
    int phend =
        (h >= pooled_height) ?
            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;
    int pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;
    int pwend =
        (w >= pooled_width) ?
            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;

    Dtype gradient = 0;
    int offset = (n * channels + c) * pooled_height * pooled_width;
    top_diff_ptr += offset;
    if (use_mask == 1) {
      mask_ptr += offset;
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    } else {
      for (int ph = phstart; ph <= phend; ph += kstride_h) {
        for (int pw = pwstart; pw <= pwend; pw += kstride_w) {
          if (top_mask[ph * pooled_width + pw] == h * width + w) {
            gradient += top_diff_ptr[ph * pooled_width + pw];
          }
        }
      }
    }
    bottom_diff[index] = gradient;
  }
}

__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, const int pad_h, const int pad_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {

    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h - pad_h;
    int wstart = pw * stride_w - pad_w;
    int hend = min(hstart + ext_kernel_h, height + pad_h);
    int wend = min(wstart + ext_kernel_w, width + pad_w);
    hstart = max(hstart, 0);
    wstart = max(wstart, 0);
    hend = min(hend, height);
    wend = min(wend, width);
    Dtype aveval = 0;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    int pool_size = 0;
    for (int h = hstart; h < hend; ++h) {
      for (int w = wstart; w < wend; ++w) {
        aveval += bottom_data_ptr[h * width + w];
        ++pool_size;
      }
    }
    top_data[index] = aveval / pool_size;
  }
}

__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w, __global Dtype* rand_idx,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    Dtype cumsum = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
      }
    }
    float thres = rand_idx[index] * cumsum;
    // Second pass: get value, and set index.
    cumsum = 0;
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        if (cumsum >= thres) {
          rand_idx[index] = ((n * channels + c) * height + h) * width + w;
          top_data[index] = bottom_data_ptr[h * width + w];
          h = hend;
          w = wend;
        }
      }
    }
  }
}

__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(
    const int nthreads, __global const Dtype* bottom_data, const int num,
    const int channels, const int height, const int width,
    const int pooled_height, const int pooled_width, const int kernel_h,
    const int kernel_w, const int ext_kernel_h, const int ext_kernel_w,
    const int stride_h, const int stride_w, const int kstride_h,
    const int kstride_w,
    __global Dtype* top_data) {

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    int pw = index % pooled_width;
    int ph = (index / pooled_width) % pooled_height;
    int c = (index / pooled_width / pooled_height) % channels;
    int n = index / pooled_width / pooled_height / channels;
    int hstart = ph * stride_h;
    int hend = min(hstart + ext_kernel_h, height);
    int wstart = pw * stride_w;
    int wend = min(wstart + ext_kernel_w, width);
    // We set cumsum to be 0 to avoid divide-by-zero problems
    Dtype cumsum = FLT_MIN;
    Dtype cumvalues = 0.;
    __global const Dtype* bottom_data_ptr = bottom_data;
    bottom_data_ptr += (n * channels + c) * height * width;
    // First pass: get sum
    for (int h = hstart; h < hend; h += kstride_h) {
      for (int w = wstart; w < wend; w += kstride_w) {
        cumsum += bottom_data_ptr[h * width + w];
        cumvalues += bottom_data_ptr[h * width + w]
            * bottom_data_ptr[h * width + w];
      }
    }
    top_data[index] = cumvalues / cumsum;
  }

}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(slice,Dtype)(const int nthreads,
                                    __global const Dtype* in_data,
                                    const int forward, const int num_slices,
                                    const int slice_size,
                                    const int bottom_slice_axis,
                                    const int top_slice_axis,
                                    const int offset_slice_axis,
                                    __global Dtype* out_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int total_slice_size = slice_size * top_slice_axis;
    const int slice_num = index / total_slice_size;
    const int slice_index = index % total_slice_size;
    const int bottom_index = slice_index
        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;
    if (forward == 1) {
      out_data[index] = in_data[bottom_index];
    } else {
      out_data[bottom_index] = in_data[index];
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif

__kernel void TEMPLATE(softmax_loss_forward,Dtype)(
    int n, __global const Dtype* prob_data, __global const Dtype* label,
    __global Dtype* loss,
    const int num, const int dim, const int spatial_dim,
    const int has_ignore_label_, const int ignore_label_,
    __global Dtype* counts) {

  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = (int) (label[n * spatial_dim + s]);
    if (has_ignore_label_ == 1 && label_value == ignore_label_) {
      loss[index] = 0;
      counts[index] = 0;
    } else {
      loss[index] = -log(
          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),
              (Dtype) FLT_MIN));
      counts[index] = 1;
    }
  }
}

__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int nthreads,
                                                    __global const Dtype* top,
                                                    __global const Dtype* label,
                                                    __global Dtype* bottom_diff,
                                                    const int num,
                                                    const int dim,
                                                    const int spatial_dim,
                                                    const int has_ignore_label_,
                                                    const int ignore_label_,
                                                    __global Dtype* counts) {

  const int channels = dim / spatial_dim;

  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    {
      const int n = index / spatial_dim;
      const int s = index % spatial_dim;
      const int label_value = (int) (label[n * spatial_dim + s]);

      if (has_ignore_label_ == 1 && label_value == ignore_label_) {
        for (int c = 0; c < channels; ++c) {
          bottom_diff[n * dim + c * spatial_dim + s] = 0;
        }
        counts[index] = 0;
      } else {
        bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
        counts[index] = 1;
      }
    }
  }
}

#ifndef __OPENCL_VERSION__
#include "header.cl"
#endif


__kernel void TEMPLATE(tile,Dtype)(const int nthreads, __global const Dtype* bottom_data,
                                   const int tile_size, const int num_tiles,
                                   const int bottom_tile_axis,
                                   __global Dtype* top_data) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size / num_tiles) % bottom_tile_axis;
    const int n = index / tile_size / num_tiles / bottom_tile_axis;
    const int bottom_index = (n * bottom_tile_axis + b) * tile_size + d;
    top_data[index] = bottom_data[bottom_index];
  }
}


__kernel void TEMPLATE(tile_backward,Dtype)(const int nthreads,
                                            __global const Dtype* top_diff,
                                            const int tile_size,
                                            const int num_tiles,
                                            const int bottom_tile_axis,
                                            __global Dtype* bottom_diff) {
  for (int index = get_global_id(0); index < nthreads;
      index += get_global_size(0)) {
    const int d = index % tile_size;
    const int b = (index / tile_size) % bottom_tile_axis;
    const int n = index / tile_size / bottom_tile_axis;
    bottom_diff[index] = 0;
    int top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;
    for (int t = 0; t < num_tiles; ++t) {
      bottom_diff[index] += top_diff[top_index];
      top_index += bottom_tile_axis * tile_size;
    }
  }
}

#endif


Note: Randomizing tests' orders with a seed of 36961 .
[==========] Running 1647 tests from 240 test cases.
[----------] Global test environment set-up.
[----------] 4 tests from BlobSimpleTest/1, where TypeParam = double
[ RUN      ] BlobSimpleTest/1.TestPointersCPUGPU
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] BlobSimpleTest/1.TestPointersCPUGPU, where TypeParam = double (0 ms)
[ RUN      ] BlobSimpleTest/1.TestLegacyBlobProtoShapeEquals
[       OK ] BlobSimpleTest/1.TestLegacyBlobProtoShapeEquals (0 ms)
[ RUN      ] BlobSimpleTest/1.TestInitialization
[       OK ] BlobSimpleTest/1.TestInitialization (0 ms)
[ RUN      ] BlobSimpleTest/1.TestReshape
[       OK ] BlobSimpleTest/1.TestReshape (0 ms)
[----------] 4 tests from BlobSimpleTest/1 (1 ms total)

[----------] 10 tests from ConcatLayerTest/2, where TypeParam = caffe::GPUDevice<float>
[ RUN      ] ConcatLayerTest/2.TestSetupChannels
[       OK ] ConcatLayerTest/2.TestSetupChannels (0 ms)
[ RUN      ] ConcatLayerTest/2.TestForwardTrivial
[       OK ] ConcatLayerTest/2.TestForwardTrivial (0 ms)
[ RUN      ] ConcatLayerTest/2.TestForwardChannels
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConcatLayerTest/2.TestForwardChannels, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConcatLayerTest/2.TestGradientChannels
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConcatLayerTest/2.TestGradientChannels, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConcatLayerTest/2.TestForwardNum
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConcatLayerTest/2.TestForwardNum, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConcatLayerTest/2.TestGradientChannelsBottomOneOnly
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConcatLayerTest/2.TestGradientChannelsBottomOneOnly, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConcatLayerTest/2.TestSetupChannelsNegativeIndexing
[       OK ] ConcatLayerTest/2.TestSetupChannelsNegativeIndexing (0 ms)
[ RUN      ] ConcatLayerTest/2.TestSetupNum
[       OK ] ConcatLayerTest/2.TestSetupNum (0 ms)
[ RUN      ] ConcatLayerTest/2.TestGradientNum
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConcatLayerTest/2.TestGradientNum, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConcatLayerTest/2.TestGradientTrivial
[       OK ] ConcatLayerTest/2.TestGradientTrivial (2 ms)
[----------] 10 tests from ConcatLayerTest/2 (4 ms total)

[----------] 1 test from HDF5OutputLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] HDF5OutputLayerTest/1.TestForward
[       OK ] HDF5OutputLayerTest/1.TestForward (3 ms)
[----------] 1 test from HDF5OutputLayerTest/1 (3 ms total)

[----------] 5 tests from DeconvolutionLayerTest/2, where TypeParam = caffe::GPUDevice<float>
[ RUN      ] DeconvolutionLayerTest/2.TestSetup
[       OK ] DeconvolutionLayerTest/2.TestSetup (0 ms)
[ RUN      ] DeconvolutionLayerTest/2.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] DeconvolutionLayerTest/2.TestGradient, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] DeconvolutionLayerTest/2.TestSimpleDeconvolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] DeconvolutionLayerTest/2.TestSimpleDeconvolution, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] DeconvolutionLayerTest/2.TestGradient3D
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] DeconvolutionLayerTest/2.TestGradient3D, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] DeconvolutionLayerTest/2.TestNDAgainst2D
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] DeconvolutionLayerTest/2.TestNDAgainst2D, where TypeParam = caffe::GPUDevice<float> (5 ms)
[----------] 5 tests from DeconvolutionLayerTest/2 (8 ms total)

[----------] 1 test from HDF5DataLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] HDF5DataLayerTest/1.TestRead
[       OK ] HDF5DataLayerTest/1.TestRead (3 ms)
[----------] 1 test from HDF5DataLayerTest/1 (3 ms total)

[----------] 8 tests from RMSPropSolverTest/2, where TypeParam = caffe::GPUDevice<float>
[ RUN      ] RMSPropSolverTest/2.TestLeastSquaresUpdateWithEverythingAccum
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestLeastSquaresUpdateWithEverythingAccum, where TypeParam = caffe::GPUDevice<float> (2 ms)
[ RUN      ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithEverythingShare
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithEverythingShare, where TypeParam = caffe::GPUDevice<float> (3 ms)
[ RUN      ] RMSPropSolverTest/2.TestSnapshot
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestSnapshot, where TypeParam = caffe::GPUDevice<float> (2 ms)
[ RUN      ] RMSPropSolverTest/2.TestSnapshotShare
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestSnapshotShare, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] RMSPropSolverTest/2.TestLeastSquaresUpdateWithEverythingAccumShare
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestLeastSquaresUpdateWithEverythingAccumShare, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithEverything
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithEverything, where TypeParam = caffe::GPUDevice<float> (2 ms)
[ RUN      ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithRmsDecay
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithRmsDecay, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithWeightDecay
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] RMSPropSolverTest/2.TestRMSPropLeastSquaresUpdateWithWeightDecay, where TypeParam = caffe::GPUDevice<float> (1 ms)
[----------] 8 tests from RMSPropSolverTest/2 (15 ms total)

[----------] 2 tests from GemmTest/0, where TypeParam = float
[ RUN      ] GemmTest/0.TestGemvCPUGPU
[       OK ] GemmTest/0.TestGemvCPUGPU (3 ms)
[ RUN      ] GemmTest/0.TestGemmCPUGPU
[       OK ] GemmTest/0.TestGemmCPUGPU (2 ms)
[----------] 2 tests from GemmTest/0 (5 ms total)

[----------] 3 tests from BlobMathTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] BlobMathTest/1.TestScaleData
[       OK ] BlobMathTest/1.TestScaleData (1 ms)
[ RUN      ] BlobMathTest/1.TestSumOfSquares
[       OK ] BlobMathTest/1.TestSumOfSquares (0 ms)
[ RUN      ] BlobMathTest/1.TestAsum
[       OK ] BlobMathTest/1.TestAsum (0 ms)
[----------] 3 tests from BlobMathTest/1 (1 ms total)

[----------] 3 tests from FilterLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] FilterLayerTest/1.TestReshape
[       OK ] FilterLayerTest/1.TestReshape (0 ms)
[ RUN      ] FilterLayerTest/1.TestGradient
[       OK ] FilterLayerTest/1.TestGradient (212 ms)
[ RUN      ] FilterLayerTest/1.TestForward
[       OK ] FilterLayerTest/1.TestForward (0 ms)
[----------] 3 tests from FilterLayerTest/1 (212 ms total)

[----------] 3 tests from PaddingLayerUpgradeTest
[ RUN      ] PaddingLayerUpgradeTest.TestImageNet
[       OK ] PaddingLayerUpgradeTest.TestImageNet (1 ms)
[ RUN      ] PaddingLayerUpgradeTest.TestSimple
[       OK ] PaddingLayerUpgradeTest.TestSimple (1 ms)
[ RUN      ] PaddingLayerUpgradeTest.TestTwoTops
[       OK ] PaddingLayerUpgradeTest.TestTwoTops (0 ms)
[----------] 3 tests from PaddingLayerUpgradeTest (2 ms total)

[----------] 5 tests from DeconvolutionLayerTest/0, where TypeParam = caffe::CPUDevice<float>
[ RUN      ] DeconvolutionLayerTest/0.TestGradient
[       OK ] DeconvolutionLayerTest/0.TestGradient (533 ms)
[ RUN      ] DeconvolutionLayerTest/0.TestGradient3D
[       OK ] DeconvolutionLayerTest/0.TestGradient3D (135 ms)
[ RUN      ] DeconvolutionLayerTest/0.TestSetup
[       OK ] DeconvolutionLayerTest/0.TestSetup (1 ms)
[ RUN      ] DeconvolutionLayerTest/0.TestNDAgainst2D
[       OK ] DeconvolutionLayerTest/0.TestNDAgainst2D (802 ms)
[ RUN      ] DeconvolutionLayerTest/0.TestSimpleDeconvolution
[       OK ] DeconvolutionLayerTest/0.TestSimpleDeconvolution (1 ms)
[----------] 5 tests from DeconvolutionLayerTest/0 (1472 ms total)

[----------] 3 tests from FilterLayerTest/3, where TypeParam = caffe::GPUDevice<double>
[ RUN      ] FilterLayerTest/3.TestReshape
[       OK ] FilterLayerTest/3.TestReshape (0 ms)
[ RUN      ] FilterLayerTest/3.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] FilterLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] FilterLayerTest/3.TestForward
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] FilterLayerTest/3.TestForward, where TypeParam = caffe::GPUDevice<double> (0 ms)
[----------] 3 tests from FilterLayerTest/3 (1 ms total)

[----------] 1 test from SolverTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] SolverTest/1.TestInitTrainTestNets
[       OK ] SolverTest/1.TestInitTrainTestNets (1 ms)
[----------] 1 test from SolverTest/1 (1 ms total)

[----------] 12 tests from ConvolutionLayerTest/2, where TypeParam = caffe::GPUDevice<float>
[ RUN      ] ConvolutionLayerTest/2.TestSimpleConvolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestSimpleConvolution, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConvolutionLayerTest/2.TestSetup
[       OK ] ConvolutionLayerTest/2.TestSetup (0 ms)
[ RUN      ] ConvolutionLayerTest/2.TestSimple3DConvolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestSimple3DConvolution, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConvolutionLayerTest/2.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestGradient, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConvolutionLayerTest/2.TestSimpleConvolutionGroup
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestSimpleConvolutionGroup, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConvolutionLayerTest/2.TestNDAgainst2D
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestNDAgainst2D, where TypeParam = caffe::GPUDevice<float> (2 ms)
[ RUN      ] ConvolutionLayerTest/2.TestSobelConvolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestSobelConvolution, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConvolutionLayerTest/2.TestGradientGroup
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestGradientGroup, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConvolutionLayerTest/2.TestGradient3D
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.TestGradient3D, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConvolutionLayerTest/2.Test1x1Gradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.Test1x1Gradient, where TypeParam = caffe::GPUDevice<float> (1 ms)
[ RUN      ] ConvolutionLayerTest/2.Test1x1Convolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.Test1x1Convolution, where TypeParam = caffe::GPUDevice<float> (0 ms)
[ RUN      ] ConvolutionLayerTest/2.Test0DConvolution
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] ConvolutionLayerTest/2.Test0DConvolution, where TypeParam = caffe::GPUDevice<float> (0 ms)
[----------] 12 tests from ConvolutionLayerTest/2 (9 ms total)

[----------] 44 tests from NeuronLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] NeuronLayerTest/1.TestAbsGradient
[       OK ] NeuronLayerTest/1.TestAbsGradient (1 ms)
[ RUN      ] NeuronLayerTest/1.TestDropoutHalf
[       OK ] NeuronLayerTest/1.TestDropoutHalf (0 ms)
[ RUN      ] NeuronLayerTest/1.TestReLUGradient
[       OK ] NeuronLayerTest/1.TestReLUGradient (2 ms)
[ RUN      ] NeuronLayerTest/1.TestDropoutGradient
[       OK ] NeuronLayerTest/1.TestDropoutGradient (2 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUParam
[       OK ] NeuronLayerTest/1.TestPReLUParam (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpGradientBase2Scale3
[       OK ] NeuronLayerTest/1.TestExpGradientBase2Scale3 (2 ms)
[ RUN      ] NeuronLayerTest/1.TestLogGradientBase2Scale3
[       OK ] NeuronLayerTest/1.TestLogGradientBase2Scale3 (3 ms)
[ RUN      ] NeuronLayerTest/1.TestExpGradient
[       OK ] NeuronLayerTest/1.TestExpGradient (2 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUInPlace
[       OK ] NeuronLayerTest/1.TestPReLUInPlace (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpLayerBase2Shift1
[       OK ] NeuronLayerTest/1.TestExpLayerBase2Shift1 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestTanHGradient
[       OK ] NeuronLayerTest/1.TestTanHGradient (3 ms)
[ RUN      ] NeuronLayerTest/1.TestExpGradientBase2Shift1Scale3
[       OK ] NeuronLayerTest/1.TestExpGradientBase2Shift1Scale3 (3 ms)
[ RUN      ] NeuronLayerTest/1.TestLogLayerBase2Shift1
[       OK ] NeuronLayerTest/1.TestLogLayerBase2Shift1 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestReLU
[       OK ] NeuronLayerTest/1.TestReLU (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpGradientBase2
[       OK ] NeuronLayerTest/1.TestExpGradientBase2 (2 ms)
[ RUN      ] NeuronLayerTest/1.TestDropoutThreeQuarters
[       OK ] NeuronLayerTest/1.TestDropoutThreeQuarters (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpGradientBase2Shift1
[       OK ] NeuronLayerTest/1.TestExpGradientBase2Shift1 (2 ms)
[ RUN      ] NeuronLayerTest/1.TestLogGradient
[       OK ] NeuronLayerTest/1.TestLogGradient (3 ms)
[ RUN      ] NeuronLayerTest/1.TestBNLLGradient
[       OK ] NeuronLayerTest/1.TestBNLLGradient (4 ms)
[ RUN      ] NeuronLayerTest/1.TestLogGradientBase2
[       OK ] NeuronLayerTest/1.TestLogGradientBase2 (3 ms)
[ RUN      ] NeuronLayerTest/1.TestSigmoidGradient
[       OK ] NeuronLayerTest/1.TestSigmoidGradient (2 ms)
[ RUN      ] NeuronLayerTest/1.TestExpLayer
[       OK ] NeuronLayerTest/1.TestExpLayer (0 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUForward
[       OK ] NeuronLayerTest/1.TestPReLUForward (0 ms)
[ RUN      ] NeuronLayerTest/1.TestLogGradientBase2Shift1
[       OK ] NeuronLayerTest/1.TestLogGradientBase2Shift1 (3 ms)
[ RUN      ] NeuronLayerTest/1.TestExpLayerBase2
[       OK ] NeuronLayerTest/1.TestExpLayerBase2 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestLogLayerBase2Shift1Scale3
[       OK ] NeuronLayerTest/1.TestLogLayerBase2Shift1Scale3 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestReLUGradientWithNegativeSlope
[       OK ] NeuronLayerTest/1.TestReLUGradientWithNegativeSlope (1 ms)
[ RUN      ] NeuronLayerTest/1.TestLogLayer
[       OK ] NeuronLayerTest/1.TestLogLayer (0 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUGradientChannelShared
[       OK ] NeuronLayerTest/1.TestPReLUGradientChannelShared (93 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUGradient
[       OK ] NeuronLayerTest/1.TestPReLUGradient (92 ms)
[ RUN      ] NeuronLayerTest/1.TestDropoutGradientTest
[       OK ] NeuronLayerTest/1.TestDropoutGradientTest (2 ms)
[ RUN      ] NeuronLayerTest/1.TestBNLL
[       OK ] NeuronLayerTest/1.TestBNLL (0 ms)
[ RUN      ] NeuronLayerTest/1.TestDropoutTestPhase
[       OK ] NeuronLayerTest/1.TestDropoutTestPhase (0 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUForwardChannelShared
[       OK ] NeuronLayerTest/1.TestPReLUForwardChannelShared (1 ms)
[ RUN      ] NeuronLayerTest/1.TestLogGradientBase2Shift1Scale3
[       OK ] NeuronLayerTest/1.TestLogGradientBase2Shift1Scale3 (2 ms)
[ RUN      ] NeuronLayerTest/1.TestTanH
[       OK ] NeuronLayerTest/1.TestTanH (0 ms)
[ RUN      ] NeuronLayerTest/1.TestSigmoid
[       OK ] NeuronLayerTest/1.TestSigmoid (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpLayerBase2Shift1Scale3
[       OK ] NeuronLayerTest/1.TestExpLayerBase2Shift1Scale3 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestPReLUConsistencyReLU
[       OK ] NeuronLayerTest/1.TestPReLUConsistencyReLU (0 ms)
[ RUN      ] NeuronLayerTest/1.TestExpLayerBase2Scale3
[       OK ] NeuronLayerTest/1.TestExpLayerBase2Scale3 (1 ms)
[ RUN      ] NeuronLayerTest/1.TestLogLayerBase2
[       OK ] NeuronLayerTest/1.TestLogLayerBase2 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestAbsVal
[       OK ] NeuronLayerTest/1.TestAbsVal (0 ms)
[ RUN      ] NeuronLayerTest/1.TestLogLayerBase2Scale3
[       OK ] NeuronLayerTest/1.TestLogLayerBase2Scale3 (0 ms)
[ RUN      ] NeuronLayerTest/1.TestReLUWithNegativeSlope
[       OK ] NeuronLayerTest/1.TestReLUWithNegativeSlope (0 ms)
[----------] 44 tests from NeuronLayerTest/1 (237 ms total)

[----------] 6 tests from MVNLayerTest/3, where TypeParam = caffe::GPUDevice<double>
[ RUN      ] MVNLayerTest/3.TestForward
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestForward, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] MVNLayerTest/3.TestGradientMeanOnly
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestGradientMeanOnly, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] MVNLayerTest/3.TestForwardAcrossChannels
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestForwardAcrossChannels, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] MVNLayerTest/3.TestGradientAcrossChannels
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestGradientAcrossChannels, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] MVNLayerTest/3.TestForwardMeanOnly
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestForwardMeanOnly, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] MVNLayerTest/3.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] MVNLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice<double> (1 ms)
[----------] 6 tests from MVNLayerTest/3 (7 ms total)

[----------] 3 tests from GPUStochasticPoolingLayerTest/1, where TypeParam = double
[ RUN      ] GPUStochasticPoolingLayerTest/1.TestStochasticTestPhase
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] GPUStochasticPoolingLayerTest/1.TestStochasticTestPhase, where TypeParam = double (1 ms)
[ RUN      ] GPUStochasticPoolingLayerTest/1.TestStochastic
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] GPUStochasticPoolingLayerTest/1.TestStochastic, where TypeParam = double (1 ms)
[ RUN      ] GPUStochasticPoolingLayerTest/1.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] GPUStochasticPoolingLayerTest/1.TestGradient, where TypeParam = double (1 ms)
[----------] 3 tests from GPUStochasticPoolingLayerTest/1 (3 ms total)

[----------] 3 tests from BlobMathTest/0, where TypeParam = caffe::CPUDevice<float>
[ RUN      ] BlobMathTest/0.TestAsum
[       OK ] BlobMathTest/0.TestAsum (0 ms)
[ RUN      ] BlobMathTest/0.TestSumOfSquares
[       OK ] BlobMathTest/0.TestSumOfSquares (0 ms)
[ RUN      ] BlobMathTest/0.TestScaleData
[       OK ] BlobMathTest/0.TestScaleData (0 ms)
[----------] 3 tests from BlobMathTest/0 (1 ms total)

[----------] 2 tests from HingeLossLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] HingeLossLayerTest/1.TestGradientL1
[       OK ] HingeLossLayerTest/1.TestGradientL1 (0 ms)
[ RUN      ] HingeLossLayerTest/1.TestGradientL2
[       OK ] HingeLossLayerTest/1.TestGradientL2 (1 ms)
[----------] 2 tests from HingeLossLayerTest/1 (1 ms total)

[----------] 2 tests from SoftmaxLayerTest/0, where TypeParam = caffe::CPUDevice<float>
[ RUN      ] SoftmaxLayerTest/0.TestForward
[       OK ] SoftmaxLayerTest/0.TestForward (0 ms)
[ RUN      ] SoftmaxLayerTest/0.TestGradient
[       OK ] SoftmaxLayerTest/0.TestGradient (174 ms)
[----------] 2 tests from SoftmaxLayerTest/0 (174 ms total)

[----------] 4 tests from NetUpgradeTest
[ RUN      ] NetUpgradeTest.TestImageNet
[       OK ] NetUpgradeTest.TestImageNet (2 ms)
[ RUN      ] NetUpgradeTest.TestAllParams
[       OK ] NetUpgradeTest.TestAllParams (1 ms)
[ RUN      ] NetUpgradeTest.TestSimple
[       OK ] NetUpgradeTest.TestSimple (1 ms)
[ RUN      ] NetUpgradeTest.TestUpgradeV1LayerType
[       OK ] NetUpgradeTest.TestUpgradeV1LayerType (3 ms)
[----------] 4 tests from NetUpgradeTest (7 ms total)

[----------] 4 tests from InnerProductLayerTest/0, where TypeParam = caffe::CPUDevice<float>
[ RUN      ] InnerProductLayerTest/0.TestForward
[       OK ] InnerProductLayerTest/0.TestForward (0 ms)
[ RUN      ] InnerProductLayerTest/0.TestGradient
[       OK ] InnerProductLayerTest/0.TestGradient (87 ms)
[ RUN      ] InnerProductLayerTest/0.TestSetUp
[       OK ] InnerProductLayerTest/0.TestSetUp (0 ms)
[ RUN      ] InnerProductLayerTest/0.TestForwardNoBatch
[       OK ] InnerProductLayerTest/0.TestForwardNoBatch (0 ms)
[----------] 4 tests from InnerProductLayerTest/0 (88 ms total)

[----------] 2 tests from HingeLossLayerTest/2, where TypeParam = caffe::GPUDevice<float>
[ RUN      ] HingeLossLayerTest/2.TestGradientL2
[       OK ] HingeLossLayerTest/2.TestGradientL2 (198 ms)
[ RUN      ] HingeLossLayerTest/2.TestGradientL1
[       OK ] HingeLossLayerTest/2.TestGradientL1 (17 ms)
[----------] 2 tests from HingeLossLayerTest/2 (216 ms total)

[----------] 2 tests from SigmoidCrossEntropyLossLayerTest/3, where TypeParam = caffe::GPUDevice<double>
[ RUN      ] SigmoidCrossEntropyLossLayerTest/3.TestGradient
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] SigmoidCrossEntropyLossLayerTest/3.TestGradient, where TypeParam = caffe::GPUDevice<double> (1 ms)
[ RUN      ] SigmoidCrossEntropyLossLayerTest/3.TestSigmoidCrossEntropyLoss
Number of kernels in program: 0
unknown file: Failure
Unknown C++ exception thrown in the test body.
[  FAILED  ] SigmoidCrossEntropyLossLayerTest/3.TestSigmoidCrossEntropyLoss, where TypeParam = caffe::GPUDevice<double> (0 ms)
[----------] 2 tests from SigmoidCrossEntropyLossLayerTest/3 (1 ms total)

[----------] 2 tests from SoftmaxLayerTest/1, where TypeParam = caffe::CPUDevice<double>
[ RUN      ] SoftmaxLayerTest/1.TestGradient
Makefile:573: recipe for target 'runtest' failed
