<!doctype html>
<html>
  <head>
    <!-- MathJax -->
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>
      Caffe | Batch Norm Layer
    </title>

    <link rel="icon" type="image/png" href="/images/caffeine-icon.png">

    <link rel="stylesheet" href="/stylesheets/reset.css">
    <link rel="stylesheet" href="/stylesheets/styles.css">
    <link rel="stylesheet" href="/stylesheets/pygment_trac.css">

    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-46255508-1', 'daggerfs.com');
    ga('send', 'pageview');
  </script>
    <div class="wrapper">
      <header>
        <h1 class="header"><a href="/">Caffe</a></h1>
        <p class="header">
          Deep learning framework by <a class="header name" href="http://bair.berkeley.edu/">BAIR</a>
        </p>
        <p class="header">
          Created by
          <br>
          <a class="header name" href="http://daggerfs.com/">Yangqing Jia</a>
          <br>
          Lead Developer
          <br>
          <a class="header name" href="http://imaginarynumber.net/">Evan Shelhamer</a>
        <ul>
          <li>
            <a class="buttons github" href="https://github.com/BVLC/caffe">View On GitHub</a>
          </li>
        </ul>
      </header>
      <section>

      <h1 id="batch-norm-layer">Batch Norm Layer</h1>

<ul>
  <li>Layer type: <code class="highlighter-rouge">BatchNorm</code></li>
  <li><a href="http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1BatchNormLayer.html">Doxygen Documentation</a></li>
  <li>Header: <a href="https://github.com/BVLC/caffe/blob/master/include/caffe/layers/batch_norm_layer.hpp"><code class="highlighter-rouge">./include/caffe/layers/batch_norm_layer.hpp</code></a></li>
  <li>CPU implementation: <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/batch_norm_layer.cpp"><code class="highlighter-rouge">./src/caffe/layers/batch_norm_layer.cpp</code></a></li>
  <li>CUDA GPU implementation: <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/layers/batch_norm_layer.cu"><code class="highlighter-rouge">./src/caffe/layers/batch_norm_layer.cu</code></a></li>
</ul>

<h2 id="parameters">Parameters</h2>

<ul>
  <li>Parameters (<code class="highlighter-rouge">BatchNormParameter batch_norm_param</code>)</li>
  <li>From <a href="https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto"><code class="highlighter-rouge">./src/caffe/proto/caffe.proto</code></a>:</li>
</ul>

<figure class="highlight"><pre><code class="language-protobuf" data-lang="protobuf"><span class="kd">message</span> <span class="nc">BatchNormParameter</span> <span class="p">{</span>
  <span class="c1">// If false, normalization is performed over the current mini-batch
</span>  <span class="c1">// and global statistics are accumulated (but not yet used) by a moving
</span>  <span class="c1">// average.
</span>  <span class="c1">// If true, those accumulated mean and variance values are used for the
</span>  <span class="c1">// normalization.
</span>  <span class="c1">// By default, it is set to false when the network is in the training
</span>  <span class="c1">// phase and true when the network is in the testing phase.
</span>  <span class="k">optional</span> <span class="kt">bool</span> <span class="na">use_global_stats</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="c1">// What fraction of the moving average remains each iteration?
</span>  <span class="c1">// Smaller values make the moving average decay faster, giving more
</span>  <span class="c1">// weight to the recent values.
</span>  <span class="c1">// Each iteration updates the moving average @f$S_{t-1}@f$ with the
</span>  <span class="c1">// current mean @f$ Y_t @f$ by
</span>  <span class="c1">// @f$ S_t = (1-\beta)Y_t + \beta \cdot S_{t-1} @f$, where @f$ \beta @f$
</span>  <span class="c1">// is the moving_average_fraction parameter.
</span>  <span class="k">optional</span> <span class="kt">float</span> <span class="na">moving_average_fraction</span> <span class="o">=</span> <span class="mi">2</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mf">.999</span><span class="p">];</span>
  <span class="c1">// Small value to add to the variance estimate so that we don't divide by
</span>  <span class="c1">// zero.
</span>  <span class="k">optional</span> <span class="kt">float</span> <span class="na">eps</span> <span class="o">=</span> <span class="mi">3</span> <span class="p">[</span><span class="k">default</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">];</span>
<span class="p">}</span></code></pre></figure>



      </section>
  </div>
  </body>
</html>
