// AUTOMATICALLY GENERATED FILE, DO NOT EDIT
#include "caffe/greentea/cl_kernels.hpp"
#include <sstream>
#include <string>
namespace caffe {
std::string activation_kernels = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define FLT_MAX 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void relu_forward_s(const int n, __global const float* in,\n                             __global float* out, float negative_slope) {\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n  }\n}\n\n__kernel void relu_forward_d(const int n, __global const double* in,\n                             __global double* out, double negative_slope) {\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n  }\n}";
std::string aux_kernels = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define FLT_MAX 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void gpu_set_s(const int n, const float alpha, __global float* y) {\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index] = alpha;\n  }\n}\n\n__kernel void gpu_set_d(const int n, const double alpha, __global double* y) {\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index] = alpha;\n  }\n}";
std::string channel_kernels = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define FLT_MAX 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void kernel_channel_max_s(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const float* data,\n                                   __global float* out) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    float maxval = -FLT_MAX;\n    for (int c = 0; c < channels; ++c) {\n      maxval = max(data[(n * channels + c) * spatial_dim + s], maxval);\n    }\n    out[index] = maxval;\n  }\n}\n\n__kernel void kernel_channel_max_d(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const double* data,\n                                   __global double* out) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    double maxval = (double) -FLT_MAX;\n    for (int c = 0; c < channels; ++c) {\n      maxval = max(data[(n * channels + c) * spatial_dim + s], maxval);\n    }\n    out[index] = maxval;\n  }\n}\n\n__kernel void kernel_channel_subtract_s(const int count, const int num,\n                                        const int channels,\n                                        const int spatial_dim,\n                                        __global const float* channel_max,\n                                        __global float* data) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int n = index / channels / spatial_dim;\n    int s = index % spatial_dim;\n    data[index] -= channel_max[n * spatial_dim + s];\n  }\n}\n\n__kernel void kernel_channel_subtract_d(const int count, const int num,\n                                        const int channels,\n                                        const int spatial_dim,\n                                        __global const double* channel_max,\n                                        __global double* data) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int n = index / channels / spatial_dim;\n    int s = index % spatial_dim;\n    data[index] -= channel_max[n * spatial_dim + s];\n  }\n}\n\n__kernel void kernel_exp_s(const int count, __global const float* data,\n                           __global float* out) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    out[index] = exp(data[index]);\n  }\n}\n\n__kernel void kernel_exp_d(const int count, __global const double* data,\n                           __global double* out) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    out[index] = exp(data[index]);\n  }\n}\n\n__kernel void kernel_channel_sum_s(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const float* data,\n                                   __global float* channel_sum) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    float sum = 0;\n    for (int c = 0; c < channels; ++c) {\n      sum += data[(n * channels + c) * spatial_dim + s];\n    }\n    channel_sum[index] = sum;\n  }\n}\n\n__kernel void kernel_channel_sum_d(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const double* data,\n                                   __global double* channel_sum) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    double sum = 0;\n    for (int c = 0; c < channels; ++c) {\n      sum += data[(n * channels + c) * spatial_dim + s];\n    }\n    channel_sum[index] = sum;\n  }\n}\n\n__kernel void kernel_channel_div_s(const int count, const int num,\n                                   const int channels, const int spatial_dim,\n                                   __global const float* channel_sum,\n                                   __global float* data) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int n = index / channels / spatial_dim;\n    int s = index % spatial_dim;\n    data[index] /= channel_sum[n * spatial_dim + s];\n  }\n}\n\n__kernel void kernel_channel_div_d(const int count, const int num,\n                                   const int channels, const int spatial_dim,\n                                   __global const double* channel_sum,\n                                   __global double* data) {\n  for (int index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int n = index / channels / spatial_dim;\n    int s = index % spatial_dim;\n    data[index] /= channel_sum[n * spatial_dim + s];\n  }\n}\n\n__kernel void kernel_channel_dot_s(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const float* data_1,\n                                   __global const float* data_2,\n                                   __global float* channel_dot) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    float dot = 0;\n    for (int c = 0; c < channels; ++c) {\n      dot += (data_1[(n * channels + c) * spatial_dim + s]\n          * data_2[(n * channels + c) * spatial_dim + s]);\n    }\n    channel_dot[index] = dot;\n  }\n}\n\n__kernel void kernel_channel_dot_d(const int num, const int channels,\n                                   const int spatial_dim,\n                                   __global const double* data_1,\n                                   __global const double* data_2,\n                                   __global double* channel_dot) {\n  for (int index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int n = index / spatial_dim;\n    int s = index % spatial_dim;\n    double dot = 0;\n    for (int c = 0; c < channels; ++c) {\n      dot += (data_1[(n * channels + c) * spatial_dim + s]\n          * data_2[(n * channels + c) * spatial_dim + s]);\n    }\n    channel_dot[index] = dot;\n  }\n}";
std::string im2col_sk_gpu_kernel = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void im2col_sk_gpu_kernel_s(const int n, __global const float* data_im,\n                                     const int height, const int width,\n                                     const int kernel_h, const int kernel_w,\n                                     const int ext_kernel_h,\n                                     const int ext_kernel_w, const int pad_h,\n                                     const int pad_w, const int stride_h,\n                                     const int stride_w, const int kstride_h,\n                                     const int kstride_w, const int height_col,\n                                     const int width_col,\n                                     __global float* data_col) {\n\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int w_out = index % width_col;\n    int h_index = index / width_col;\n    int h_out = h_index % height_col;\n    int channel_in = h_index / height_col;\n    int channel_out = channel_in * kernel_h * kernel_w;\n    int h_in = h_out * stride_h - pad_h;\n    int w_in = w_out * stride_w - pad_w;\n    __global float* data_col_ptr = data_col;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const float* data_im_ptr = data_im;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int i = 0; i < ext_kernel_h; i += kstride_h) {\n      for (int j = 0; j < ext_kernel_w; j += kstride_w) {\n        int h = h_in + i;\n        int w = w_in + j;\n        *data_col_ptr =\n            (h >= 0 && w >= 0 && h < height && w < width) ?\n                data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n\n}\n\n__kernel void im2col_sk_gpu_kernel_d(const int n,\n                                     __global const double* data_im,\n                                     const int height, const int width,\n                                     const int kernel_h, const int kernel_w,\n                                     const int ext_kernel_h,\n                                     const int ext_kernel_w, const int pad_h,\n                                     const int pad_w, const int stride_h,\n                                     const int stride_w, const int kstride_h,\n                                     const int kstride_w, const int height_col,\n                                     const int width_col,\n                                     __global double* data_col) {\n\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int w_out = index % width_col;\n    int h_index = index / width_col;\n    int h_out = h_index % height_col;\n    int channel_in = h_index / height_col;\n    int channel_out = channel_in * kernel_h * kernel_w;\n    int h_in = h_out * stride_h - pad_h;\n    int w_in = w_out * stride_w - pad_w;\n    __global double* data_col_ptr = data_col;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const double* data_im_ptr = data_im;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int i = 0; i < ext_kernel_h; i += kstride_h) {\n      for (int j = 0; j < ext_kernel_w; j += kstride_w) {\n        int h = h_in + i;\n        int w = w_in + j;\n        *data_col_ptr =\n            (h >= 0 && w >= 0 && h < height && w < width) ?\n                data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n\n}";
std::string pooling_sk_kernels = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define FLT_MAX 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void max_pool_forward_sk_s(const int nthreads,\n                                    __global const float* bottom_data,\n                                    const int num, const int channels,\n                                    const int height, const int width,\n                                    const int pooled_height,\n                                    const int pooled_width, const int kernel_h,\n                                    const int kernel_w, const int ext_kernel_h,\n                                    const int ext_kernel_w, const int stride_h,\n                                    const int stride_w, const int kstride_h,\n                                    const int kstride_w, const int pad_h,\n                                    const int pad_w, __global float* top_data,\n                                    __global int* mask,\n                                    __global float* top_mask) {\n\n  for (int index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int pw = index % pooled_width;\n    int ph = (index / pooled_width) % pooled_height;\n    int c = (index / pooled_width / pooled_height) % channels;\n    int n = index / pooled_width / pooled_height / channels;\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + ext_kernel_h, height);\n    int wend = min(wstart + ext_kernel_w, width);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    float maxval = -FLT_MAX;\n    int maxidx = -1;\n    bottom_data += (n * channels + c) * height * width;\n    for (int h = hstart; h < hend; h += kstride_h) {\n      for (int w = wstart; w < wend; w += kstride_w) {\n        if (bottom_data[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_data[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (mask) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n\n__kernel void max_pool_forward_sk_d(const int nthreads,\n                                    __global const double* bottom_data,\n                                    const int num, const int channels,\n                                    const int height, const int width,\n                                    const int pooled_height,\n                                    const int pooled_width, const int kernel_h,\n                                    const int kernel_w, const int ext_kernel_h,\n                                    const int ext_kernel_w, const int stride_h,\n                                    const int stride_w, const int kstride_h,\n                                    const int kstride_w, const int pad_h,\n                                    const int pad_w, __global double* top_data,\n                                    __global int* mask,\n                                    __global double* top_mask) {\n\n  for (int index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int pw = index % pooled_width;\n    int ph = (index / pooled_width) % pooled_height;\n    int c = (index / pooled_width / pooled_height) % channels;\n    int n = index / pooled_width / pooled_height / channels;\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + ext_kernel_h, height);\n    int wend = min(wstart + ext_kernel_w, width);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    double maxval = -FLT_MAX;\n    int maxidx = -1;\n    bottom_data += (n * channels + c) * height * width;\n    for (int h = hstart; h < hend; h += kstride_h) {\n      for (int w = wstart; w < wend; w += kstride_w) {\n        if (bottom_data[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_data[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (mask) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}";
std::string softmax_loss_gpu = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define FLT_MIN 0\n#endif\n\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n\n__kernel void softmax_loss_forward_gpu_s(int n, __global const float* prob_data,\n                                         __global const float* label,\n                                         __global float* loss, const int num,\n                                         const int dim, const int spatial_dim,\n                                         const int has_ignore_label_,\n                                         const int ignore_label_,\n                                         __global float* counts) {\n\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    const int n = index / spatial_dim;\n    const int s = index % spatial_dim;\n    const int label_value = (int) (label[n * spatial_dim + s]);\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      loss[index] = 0;\n      counts[index] = 0;\n    } else {\n      loss[index] = -log(\n          max((float) (prob_data[n * dim + label_value * spatial_dim + s]),\n              (float) FLT_MIN));\n      counts[index] = 1;\n    }\n  }\n\n}\n\n__kernel void softmax_loss_forward_gpu_d(int n,\n                                         __global const double* prob_data,\n                                         __global const double* label,\n                                         __global double* loss, const int num,\n                                         const int dim, const int spatial_dim,\n                                         const int has_ignore_label_,\n                                         const int ignore_label_,\n                                         __global double* counts) {\n\n  for (int index = get_global_id(0); index < n; index += get_global_size(0)) {\n    const int n = index / spatial_dim;\n    const int s = index % spatial_dim;\n    const int label_value = (int) (label[n * spatial_dim + s]);\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      loss[index] = 0;\n      counts[index] = 0;\n    } else {\n      loss[index] = -log(\n          max((double) (prob_data[n * dim + label_value * spatial_dim + s]),\n              (double) FLT_MIN));\n      counts[index] = 1;\n    }\n  }\n\n}";
viennacl::ocl::program & RegisterKernels(viennacl::ocl::context &ctx) {
  std::stringstream ss;
  ss << activation_kernels << "\n\n";
  ss << aux_kernels << "\n\n";
  ss << channel_kernels << "\n\n";
  ss << im2col_sk_gpu_kernel << "\n\n";
  ss << pooling_sk_kernels << "\n\n";
  ss << softmax_loss_gpu << "\n\n";
  std::string kernel_string = ss.str();
  const char* kernel_program = kernel_string.c_str();
  viennacl::ocl::program &program = ctx.add_program(kernel_program,"kernel_program");
  return program;
}
}
